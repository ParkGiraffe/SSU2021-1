{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"usc_cnn_ex.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"7prG1BISon3O"},"source":["# **Urban Sound Classification**"]},{"cell_type":"code","metadata":{"id":"x-hwpq6don3V"},"source":["import IPython.display as ipd\n","import librosa\n","import librosa.display\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hF0yA7aUon3o"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data_utils"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9D6ifd1uon3Y","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1623737639105,"user_tz":-540,"elapsed":1903,"user":{"displayName":"Park YoSep","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT_E-VFjPtEAlEsu559PDDy3PrYNUs2fbAiAvsQA=s64","userId":"14662737176709564144"}},"outputId":"15e44abb-2542-4c12-e4de-221269160309"},"source":["path = '/content/drive/MyDrive/Colab Notebooks/디지털사운드/13주차/'\n","# path = 'D:\\data/'\n","path_train = '/content/drive/MyDrive/Colab Notebooks/디지털사운드/13주차/Train/'\n","df = pd.read_csv(path + 'train.csv')\n","test_df = pd.read_csv(path + 'test.csv')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>siren</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>street_music</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>drilling</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>siren</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>dog_bark</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5430</th>\n","      <td>8725</td>\n","      <td>engine_idling</td>\n","    </tr>\n","    <tr>\n","      <th>5431</th>\n","      <td>8726</td>\n","      <td>dog_bark</td>\n","    </tr>\n","    <tr>\n","      <th>5432</th>\n","      <td>8727</td>\n","      <td>engine_idling</td>\n","    </tr>\n","    <tr>\n","      <th>5433</th>\n","      <td>8728</td>\n","      <td>engine_idling</td>\n","    </tr>\n","    <tr>\n","      <th>5434</th>\n","      <td>8729</td>\n","      <td>air_conditioner</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5435 rows × 2 columns</p>\n","</div>"],"text/plain":["        ID            Class\n","0        0            siren\n","1        1     street_music\n","2        2         drilling\n","3        3            siren\n","4        4         dog_bark\n","...    ...              ...\n","5430  8725    engine_idling\n","5431  8726         dog_bark\n","5432  8727    engine_idling\n","5433  8728    engine_idling\n","5434  8729  air_conditioner\n","\n","[5435 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p61a6VUKqTfp","executionInfo":{"status":"ok","timestamp":1623737632825,"user_tz":-540,"elapsed":19942,"user":{"displayName":"Park YoSep","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT_E-VFjPtEAlEsu559PDDy3PrYNUs2fbAiAvsQA=s64","userId":"14662737176709564144"}},"outputId":"58e1fdf3-514f-4487-a71e-b37839fbe6cc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s0EWgsGK7-da"},"source":["# **데이터 전처리**"]},{"cell_type":"markdown","metadata":{"id":"lxrZb1xI8BtQ"},"source":["**범주형(Categorical) 데이터셋으로 변환**"]},{"cell_type":"code","metadata":{"id":"o1uBrIqton3g","colab":{"base_uri":"https://localhost:8080/","height":419},"outputId":"758017d5-c87c-4764-9f0d-364124ba872c"},"source":["# Converting classes into numeric format\n","df['numeric_class'] = df['Class'].astype('category').cat.codes\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Class</th>\n","      <th>numeric_class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>siren</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>street_music</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>drilling</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>siren</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>dog_bark</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5430</th>\n","      <td>8725</td>\n","      <td>engine_idling</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>5431</th>\n","      <td>8726</td>\n","      <td>dog_bark</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>5432</th>\n","      <td>8727</td>\n","      <td>engine_idling</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>5433</th>\n","      <td>8728</td>\n","      <td>engine_idling</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>5434</th>\n","      <td>8729</td>\n","      <td>air_conditioner</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5435 rows × 3 columns</p>\n","</div>"],"text/plain":["        ID            Class  numeric_class\n","0        0            siren              8\n","1        1     street_music              9\n","2        2         drilling              4\n","3        3            siren              8\n","4        4         dog_bark              3\n","...    ...              ...            ...\n","5430  8725    engine_idling              5\n","5431  8726         dog_bark              3\n","5432  8727    engine_idling              5\n","5433  8728    engine_idling              5\n","5434  8729  air_conditioner              0\n","\n","[5435 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"5u60fDFm8G-K"},"source":["**Train Dataset과 Validation Dataset으로 나누기**"]},{"cell_type":"code","metadata":{"id":"9yzjsK26on3i"},"source":["def train_val_split(df):\n","    train_df = pd.DataFrame(columns = df.columns)\n","    val_df = pd.DataFrame(columns = df.columns)\n","\n","    train_df = df[:int(df['ID'].count()*0.8)]\n","    val_df = df[4348:]\n","        \n","    return train_df, val_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BL5PzG3won3k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"99a70526-ef67-471a-9e3b-8bd8dbfe382d"},"source":["train_df, val_df = train_val_split(df)\n","train_df.shape, val_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4348, 3), (1087, 3))"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"Ipmnop1f8MzH"},"source":["**진행 시각화(Visualization)**"]},{"cell_type":"code","metadata":{"id":"XWwFgp2e21N0"},"source":["import cv2\n","import sys\n","\n","def drawProgressBar(current, total, string = '', barLen = 20):\n","\n","    percent = current/total\n","    arrow = \">\"\n","    if percent == 1:\n","        arrow = \"\"\n","   \n","    sys.stdout.write(\"\\r\")\n","    sys.stdout.write(\"Progress: [{:<{}}] {}/{}\".format(\"=\" * int(barLen * percent) + arrow, \n","                                                         barLen, current, total) + string)\n","    sys.stdout.flush()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvD10v7P8VM-"},"source":["**동일한 크기의 오디오 클립 만들기**"]},{"cell_type":"code","metadata":{"id":"9TvDth2Non3Z"},"source":["def get_audio_same_len(wav, sr):\n","    if wav.shape[0] < 4 * sr:\n","\n","      #4초보다 작은 것은 4초로 확대하고 복제.\n","        wav = np.pad(wav, int(np.ceil((4 * sr - wav.shape[0])/2)), mode = 'reflect')\n","    \n","    #4초까지 잘라내기\n","    wav = wav[:4 * sr]\n","    \n","    return wav"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgIWXws_8Yjd"},"source":["**스펙트로그램 만들기**"]},{"cell_type":"code","metadata":{"id":"z60h9Qn-on3a"},"source":["def get_melspectrogram_db(wav, sr):\n","\n","  #mel 스펙트로그램 만들기\n","  \n","    wav = get_audio_same_len(wav, sr)\n","        \n","    spec = librosa.feature.melspectrogram(wav, sr, n_fft = 2048, hop_length = 512, \n","                          n_mels = 128, fmin = 20, fmax = 8300)\n","    \n","    spec = librosa.power_to_db(spec, top_db = 80)\n","    return spec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mv3i4Jff8fe7"},"source":["**표준화와 정규화**"]},{"cell_type":"code","metadata":{"id":"RQNJmNrvGIFe"},"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","\n","def standard_norm(spec):\n","    mMscaler = MinMaxScaler()\n","    sdscaler = StandardScaler()\n","\n","    spec = sdscaler.fit_transform(spec)\n","    spec = mMscaler.fit_transform(spec)\n","    spec_scaled = spec*255\n","\n","    return spec_scaled"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hqNV-gPEon3p"},"source":["BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X7P-Ro5Q8pYa"},"source":["**음성 데이터 로딩(loading)**"]},{"cell_type":"code","metadata":{"id":"fUv3vzNqon3p"},"source":["def load_data(df):\n","    audio_data = []\n","    sample_rates = []\n","    labels = []\n","    \n","    tot = len(df)\n","    curr = 0\n","    \n","    for idx in df.index:\n","        try:\n","            file_name = str(df['ID'][idx]) + '.wav'\n","            #CNN\n","            wav, sr = librosa.load('/content/drive/MyDrive/Colab Notebooks/data/Train/' + file_name)\n","            \n","            wav = get_audio_same_len(wav, sr)\n","    \n","            audio_data.append(wav)\n","            sample_rates.append(sr)\n","            \n","            labels.append(df['numeric_class'][idx])\n","            \n","            curr += 1\n","            drawProgressBar(curr, tot, barLen = 40)\n","        \n","        #예외처리\n","        except KeyboardInterrupt:\n","            print('KeyBoardInterrupt')\n","            break\n","        \n","        except Exception:\n","            print(\"Couldn't read file\", df['ID'][idx])\n","            curr += 1\n","            \n","    print('\\n')\n","    return np.stack(audio_data, axis = 0), np.array(sample_rates), np.array(labels)\n","    #3가지 형태로 저장됨. 오디오 데이터|샘플레이트|라벨"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3j52YKYon3q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5fa75003-e101-46b8-cbf4-9d955e967122"},"source":["train_data, train_sr, train_labels = load_data(train_df)\n","#3가지로 저장된 것을 각각 분배.\n","val_data, val_sr, val_labels = load_data(val_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Progress: [========================================] 4348/4348\n","\n","Progress: [========================================] 1087/1087\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mvq55ELOon3r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1478632-5aa7-4716-cea9-6064ae3be0e9"},"source":["train_data.shape, val_data.shape\n","\n","#88200 = 4 * 샘플링레이트 = 타임값 (ANN&DNN)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4348, 88200), (1087, 88200))"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"u_vIaikx81Gx"},"source":["**데이터 변환(Coversion)과 Tensor Dataset 구축**"]},{"cell_type":"code","metadata":{"id":"zhfmGIhVon3s"},"source":["# Convert numpy arrays to torch tensors | numpy에서 tensor로\n","train_data = torch.from_numpy(train_data)\n","train_labels = torch.from_numpy(train_labels).long()\n","\n","val_data = torch.from_numpy(val_data)\n","val_labels = torch.from_numpy(val_labels).long()\n","\n","# Create data loaders\n","train_data = data_utils.TensorDataset(train_data, train_labels)\n","val_data = data_utils.TensorDataset(val_data, val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G70cw58yon3u"},"source":["## **Convolutional Neural Network(CNN)** on Spectrogram Images"]},{"cell_type":"code","metadata":{"id":"SjVw5Fjjon3u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31e4fa8e-198e-4dd4-e9cd-87b2e85adf74"},"source":["set(train_sr), set(val_sr)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({22050}, {22050})"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"2PM54NWmon3w"},"source":["train_sr = 22050\n","val_sr = 22050"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hBI3mSShAmRB"},"source":["**DataLoader 구축하기**"]},{"cell_type":"code","metadata":{"id":"x_T_W_w8on3w"},"source":["def get_spectrogram_loader(audio_data, sr, batch_size, shuffle = False):\n","\n","    hop_length = 512 # 샘플의 수\n","    n_fft = 2048 # 윈도우 # spectral resolution / window length\n","\n","    audio_spec_img = []\n","    labels = []\n","    curr = 0\n","    tot = len(audio_data)\n","\n","    for wav, label in audio_data:\n","        spec_img = standard_norm(get_melspectrogram_db(wav.numpy(), sr)) #스펙트로그램 만들기 && 표준화와 정규화\n","        spec_img = np.expand_dims(spec_img, axis = 0) #데이터의 차원 확대. 확대하지 않으면 데이터의 차원이 맞지 않음. => 대괄호 하나 더 만들어줌.\n","        audio_spec_img.append(spec_img) #각각 스펙트로그램 이미지를 추가\n","        labels.append(label)\n","\n","        curr += 1\n","        drawProgressBar(curr, tot, barLen = 40)\n","\n","    audio_spec_img = torch.Tensor(audio_spec_img) #스펙트로그램 이미지를 텐서로 저장\n","    audio_spec_img = audio_spec_img / 255\n","    \n","    labels = torch.Tensor(labels).long()\n","\n","    audio_spec_img = data_utils.TensorDataset(audio_spec_img, labels) #스펙트로그램 이미지와 라벨을 텐서 데이터셋으로 모음.\n","    #audio_loader = data_utils.DataLoader(audio_spec_img, batch_size = batch_size, shuffle = shuffle)\n","    \n","    #return audio_loader\n","    return audio_spec_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoC6gbwMon3x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a15c7ad9-bfc9-48cf-ebda-e9ffff6baa87"},"source":["train_spec_dataset = get_spectrogram_loader(train_data, train_sr, BATCH_SIZE, shuffle = True)\n","\n","#train_data(tensor dataset)를 함수를 거치면서 spectrogram 이미지 값이 포함됨 tensor 데이터셋으로 저장"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Progress: [========================================] 4348/4348"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pb_44wP1tVqf"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrGFtjkQF18A","outputId":"c779012d-1a5a-42f2-ee6c-ad1572fba18a"},"source":["train_spec_dataset[0][0].size()\n","\n","#[채널수 , 높이, 넓이] = 이미지 값으로 변경됨(audio_spec_img). 이것이 input 값 | ANN에서는 타임값이였는데 CNN에서는 이미지 값으로 \n","#채널 : 1ch :스펙트로그램은 rgb값 안 넣어도 흑백이어도 상관 없어서 1채널로 저장."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 128, 173])"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"id":"FkNH9AUS_vf3"},"source":["train_loader = data_utils.DataLoader(train_spec_dataset, batch_size = BATCH_SIZE, shuffle = False)\n","#모델에 입력 가능한 형태인 loader 제작."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3px0kV9on3y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0042d865-56a3-45f6-bdb0-ba2bbce6a9fa"},"source":["#위와 똑같은 과정으로 valid dataset 제작.\n","val_spec_dataset  = get_spectrogram_loader(val_data, val_sr, BATCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Progress: [========================================] 1087/1087"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0-Fh207AVE-","outputId":"f705c365-2553-4eec-c4bc-e0b98ca28f75"},"source":["val_spec_dataset[0][0].size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 128, 173])"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"code","metadata":{"id":"a_wLVJpCAXfb"},"source":["val_loader = data_utils.DataLoader(train_spec_dataset, batch_size = BATCH_SIZE, shuffle = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OK92j_VPAowA"},"source":["**CNN 모델 구축하기**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1A-0cxDLHZmA","outputId":"90c38c12-1b9f-4c2a-cf37-fd5d5ba930e6"},"source":["# 파라미터 설정 방법\n","input = torch.Tensor(1,1,128,173) #이미지\n","conv1 = nn.Conv2d(1, 8, (5, 6)) #1ch이 들어가서 8ch이 나오고 / kernel_size가 5,6 (가로,세로) *가로 세로 동일 시 괄호 필요 없음. 한 번만 쓰면 됨.\n","conv2 = nn.Conv2d(8, 16, 3 #이전에 나온 수와 동일한 값을 집어 넣는 걸로 설정해야 함. \n","pool =  nn.MaxPool2d(2) #pool연산 -> 특징이 모아짐 ->크기 감소\n","out=conv1(input)\n","out=pool(out)\n","\n","out.shape\n","#1,128,173 - > 8, 62, 84\n","#밑의 과정도 동일"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 8, 62, 84])"]},"metadata":{"tags":[]},"execution_count":186}]},{"cell_type":"code","metadata":{"id":"5xU0fS3Pon30"},"source":["NUM_CLASSES = 10\n","LEARNING_RATE = 0.001\n","EPOCHS = 10\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","\n","        #CNN은 이미지 사이즈가 일관성을 가지고 줄어들어야 함. 따라서 [1, 128,173] && 64*10*15 && NUM_CLASSES는 함부로 바꿔서는 안 됨.\n","        #위에 파라미터 설정 방법 탭을 이용해서 미리 계산 가능.\n","        #pytorch는 직접 계산해야 함.. keras는 자동으로 해줌.\n","        \n","        # Layer 1, Input shape (1, 128, 173) ->  Output shape (8, 62, 84)\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (5, 6)), \n","            nn.ReLU(), #이미지 크기를 변화시키지는 않음. 그래서 Conv2d 사용.\n","            nn.MaxPool2d(kernel_size = (2, 2)))\n","        \n","        # Layer 2, Input shape (8, 62, 84) -> Output shape (16, 30, 41)\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3)), \n","            nn.ReLU(), \n","            nn.MaxPool2d(kernel_size = (2, 2)))\n","        \n","        # Layer 3, Input shape (16, 30, 41) -> Output shape (64, 10, 15)\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (6, 7)), \n","            nn.ReLU(), \n","            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (6, 6)), \n","            nn.ReLU(), \n","            nn.MaxPool2d(kernel_size = (2, 2)))\n","        \n","        #flatten시킨 후 이어지는 과정\n","        #폈을 때의 값이 64*10*15가 나오는데, 코딩을 처음 짤 때는 예측을 해야 한다.\n","\n","        # Fully Connected layer 1, Input features 64 * 10 * 15 -> Output features 512\n","        self.fc1 = nn.Linear(in_features = 64 * 10 * 15, out_features = 512)\n","        \n","        # Fully Connected layer 2, Input features 512 -> Output features 256\n","        self.fc2 = nn.Linear(in_features = 512, out_features = 256)\n","        \n","        # Fully Connected layer 3, Input features 256 -> Output features 128\n","        self.fc3 = nn.Linear(in_features = 256, out_features = 128)\n","        \n","        # Fully Connected layer 4, Input features 128 -> Output features 10\n","        self.fc4 = nn.Linear(in_features = 128, out_features = NUM_CLASSES)\n","        #최종 분류.\n","        \n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        \n","        x = x.view(-1, self.num_flat_features(x))\n","        \n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        \n","        return x\n","    \n","    def num_flat_features(self, x):\n","        size = x.size()[1:]\n","        n_features = 1\n","        for s in size:\n","            n_features = n_features * s\n","        \n","        return n_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PFHouSPHAyWM"},"source":["**CNN 클래스 불러오기**"]},{"cell_type":"code","metadata":{"id":"zBFdhu0son31"},"source":["model = ConvNet()\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsV4nhxqA2CW"},"source":["**학습(Training)**"]},{"cell_type":"code","metadata":{"id":"dlFhmw59on33","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0b90cd8-af07-49dd-dc8d-3d36c69fd500"},"source":["THRESHOLD = 0.001 \n","num_train_batches = len(train_loader)\n","\n","for epoch in range(EPOCHS):\n","    print(\"Epoch \" + str(epoch + 1) + \":\")\n","    \n","    for i, batch in enumerate(train_loader):\n","        \n","        data, labels = batch\n","        \n","        outputs = model(data)\n","        loss = loss_fn(outputs, labels)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total = labels.size(0)\n","        _, predicted = torch.max(outputs, dim = 1)\n","        correct = (predicted == labels).sum().item()\n","        accuracy = correct / total\n","        \n","        drawProgressBar((i + 1), num_train_batches, \n","                              '\\t loss: {:.4f} \\t acc: {:.4f}'.format(round(loss.item(), 4), round(accuracy, 4)))\n","    \n","    print('\\n\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1:\n","Progress: [====================] 136/136\t loss: 1.7394 \t acc: 0.2857\n","\n","\n","Epoch 2:\n","Progress: [====================] 136/136\t loss: 1.5513 \t acc: 0.5000\n","\n","\n","Epoch 3:\n","Progress: [====================] 136/136\t loss: 1.1755 \t acc: 0.6429\n","\n","\n","Epoch 4:\n","Progress: [====================] 136/136\t loss: 0.9580 \t acc: 0.6786\n","\n","\n","Epoch 5:\n","Progress: [====================] 136/136\t loss: 0.8415 \t acc: 0.8214\n","\n","\n","Epoch 6:\n","Progress: [====================] 136/136\t loss: 0.6005 \t acc: 0.8571\n","\n","\n","Epoch 7:\n","Progress: [====================] 136/136\t loss: 0.6528 \t acc: 0.8214\n","\n","\n","Epoch 8:\n","Progress: [====================] 136/136\t loss: 0.3204 \t acc: 0.9286\n","\n","\n","Epoch 9:\n","Progress: [====================] 136/136\t loss: 0.3002 \t acc: 0.8929\n","\n","\n","Epoch 10:\n","Progress: [====================] 136/136\t loss: 0.1883 \t acc: 0.9286\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MMIHpmXKA-Qf"},"source":["**평가**"]},{"cell_type":"code","metadata":{"id":"XezvIqL4on3t"},"source":["def evaluate(model, test_loader):\n","\n","    model.eval()\n","    num_test_batches = len(test_loader)\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        total_loss = 0\n","        for i, batch in enumerate(test_loader):\n","            inputs, labels = batch\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, dim = 1)\n","            loss = loss_fn(outputs, labels)\n","            total_loss += loss.item()\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            drawProgressBar((i+1), num_test_batches)\n","        \n","        accuracy = correct/total\n","        test_loss = total_loss/num_test_batches\n","    \n","    return accuracy, test_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVQiqJGhon35","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7412f584-e953-4136-a36f-f54e807336ed"},"source":["val_acc, val_loss = evaluate(model, val_loader)\n","\n","print(\"\\n\\nValidation accuracy: {:.4f}\".format(round(val_acc, 4)))\n","print(\"Validation loss: {:.4f}\".format(round(val_loss, 4)))\n","\n","# 정확도가 20% -> 90%"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Progress: [====================] 136/136\n","\n","Validation accuracy: 0.9121\n","Validation loss: 0.2446\n"],"name":"stdout"}]}]}